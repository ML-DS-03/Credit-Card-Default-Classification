{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "file_path=r'C:\\Users\\Marco\\Desktop\\TU Dublin\\Programming for Big Data - H6018\\2nd Assignment\\data'\n",
    "file_name='/credit_card_default.xls'\n",
    "\n",
    "df = pd.read_excel(file_path+file_name, encoding = \"ISO-8859-1\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking duplicates for ID and drop it\n",
    "df.ID.duplicated().sum()\n",
    "df.drop(['ID'], axis=1, inplace=True)\n",
    "#rename columns\n",
    "df = df.rename(columns = {'default payment next month' : 'def_next_month', \n",
    "                          'PAY_0' : 'PAY_1'})\n",
    "df.def_next_month.value_counts()\n",
    "#Correct data inconsistencies \n",
    "# MARRIAGE = 0 is deleted\n",
    "df = df.drop(df[df['MARRIAGE']==0].index)\n",
    "# EDUCATION = 0, 5 and 6 are deleted\n",
    "df = df.drop(df[df['EDUCATION']==0].index)\n",
    "df = df.drop(df[df['EDUCATION']==5].index)\n",
    "df = df.drop(df[df['EDUCATION']==6].index)\n",
    "#Fixing PAY variables\n",
    "for att in ['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n",
    "  # categories -2,-1 are grouped into a single class -1: pay duty   \n",
    "  filter = (df[att] == -2) | (df[att] == -1) \n",
    "  df.loc[filter, att] = -1\n",
    "  df[att] = df[att].astype('int64')\n",
    "  filter = (df[att] >= 0)\n",
    "  df.loc[filter, att] = df.loc[filter, att] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'category' type to categorical attributes\n",
    "for att in ['SEX', 'EDUCATION', 'MARRIAGE']:\n",
    "  df[att] = df[att].astype('category')\n",
    "\n",
    "# one-hot encoding\n",
    "df_encoded=pd.concat([pd.get_dummies(df['SEX'], prefix='SEX'),\n",
    "                pd.get_dummies(df['EDUCATION'], prefix='EDUCATION'), \n",
    "                pd.get_dummies(df['MARRIAGE'], prefix='MARRIAGE'),\n",
    "                df],axis=1)\n",
    "# drop original columns\n",
    "df_encoded.drop(['EDUCATION'],axis=1, inplace=True)\n",
    "df_encoded.drop(['SEX'],axis=1, inplace=True)\n",
    "df_encoded.drop(['MARRIAGE'],axis=1, inplace=True)\n",
    "\n",
    "# drop response variable and the hot encoded variables\n",
    "df_drop_var = df_encoded.drop(['def_next_month', 'SEX_1','SEX_2','EDUCATION_1','EDUCATION_2','EDUCATION_3','EDUCATION_4',\n",
    "                               'MARRIAGE_1','MARRIAGE_2','MARRIAGE_3'],axis=1)\n",
    "\n",
    "df_scaled = df_drop_var/df_drop_var.std()\n",
    "\n",
    "#concatenating the encoded variables with the scaled variables\n",
    "df_encoded_only = df_encoded.iloc[:,:9]\n",
    "df_prep = pd.concat([df_encoded_only, df_scaled, df.def_next_month],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation and Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the data into train, validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Train, test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_NOT_train, y_train, y_NOT_train = train_test_split(df_prep.drop(['def_next_month'],axis=1),\n",
    "                                                                        df_prep.def_next_month,test_size=0.3,\n",
    "                                                                        random_state=101)\n",
    "\n",
    "# split 30% groups into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_NOT_train,y_NOT_train,test_size=0.5,\n",
    "                                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the shapes to make sure that everything has worked out okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20720, 29)\n",
      "(4440, 29)\n",
      "(4441, 29)\n",
      "(4440,)\n",
      "(4441,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "The following models will be explored:\n",
    "\n",
    "* Random Forest\n",
    "\n",
    "* KNN\n",
    "* Decision Tree\n",
    "* Decision Tree with Resampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start applying Random Forest with gridSearch for hyperparameter tuning, we can observe that model can be improved in prediciting Default Group. Especially the precision results. This is probably due to the unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average = 'macro'),\n",
    "           'recall': make_scorer(recall_score, average = 'macro'),\n",
    "           'f1': make_scorer(f1_score, average = 'macro')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [4, 5, 6, 7, 8],\n",
       "                         'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
       "                         'n_estimators': [10, 30, 50, 100]},\n",
       "             refit='f1',\n",
       "             scoring={'accuracy': make_scorer(accuracy_score),\n",
       "                      'f1': make_scorer(f1_score, average=macro),\n",
       "                      'precision': make_scorer(precision_score, average=macro),\n",
       "                      'recall': make_scorer(recall_score, average=macro)})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "\n",
    "grid_values = {'n_estimators': [10, 30, 50, 100],\n",
    "               'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n",
    "               'max_depth' : [4,5,6,7,8],\n",
    "              }\n",
    "\n",
    "\n",
    "grid_search_rfc = GridSearchCV(forest, param_grid = grid_values, scoring = scoring, refit='f1',)\n",
    "grid_search_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 8, 'max_features': 1.0, 'n_estimators': 100}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(max_depth=8, max_features=1.0, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, max_features=1.0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRF = forest.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.69      0.75      3156\n",
      "           1       0.19      0.31      0.24       734\n",
      "\n",
      "    accuracy                           0.62      3890\n",
      "   macro avg       0.50      0.50      0.49      3890\n",
      "weighted avg       0.69      0.62      0.65      3890\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest')\n",
    "print(classification_report(predRF,y_test))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to apply a KNN model using repeated stratified cross validation in order to preserve the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2, 10)\n",
    "k_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7811293436293438,\n",
       " 0.7698841698841699,\n",
       " 0.7907818532818532,\n",
       " 0.788465250965251,\n",
       " 0.7951254826254825,\n",
       " 0.7942084942084942,\n",
       " 0.8001447876447877,\n",
       " 0.7974903474903476]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the validity of this model with the training and validation set, and Cohen's Kappa. We will select 6 neighbors - though it doesn't seem to matter too much. \n",
    "\n",
    "Commonly, KNN is fitted with the training set and used to predict the validation set. Then a new KNN is fitted to the validation set, and the two models are compared to see how generaliseable the KNN is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=6)\n",
    "knn1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3309  704]\n",
      " [ 174  253]]\n"
     ]
    }
   ],
   "source": [
    "preds = knn1.predict(X_val)\n",
    "print(confusion_matrix(preds, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors = 6)\n",
    "knn2.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3403  678]\n",
      " [  80  279]]\n"
     ]
    }
   ],
   "source": [
    "preds_VAL = knn2.predict(X_val)\n",
    "print(confusion_matrix(preds_VAL, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by comparing the two confusion matrices, we can see that the numbers are similar. There is a small increase in accuracy in the model trained on the validation set, but it is low enough to say that the clustering is generaliseable. \n",
    "\n",
    "We can also check the predictions with Cohen's Kappa. We get 47% agreement, which is low score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4699763198287036"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(preds_VAL, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can apply a decision tree model, we see that precision, recal and F1 are higher than in the Random Forest with gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1 = DecisionTreeClassifier()\n",
    "tree1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = tree1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81      3320\n",
      "           1       0.43      0.36      0.39      1120\n",
      "\n",
      "    accuracy                           0.72      4440\n",
      "   macro avg       0.61      0.60      0.60      4440\n",
      "weighted avg       0.70      0.72      0.71      4440\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Decision Tree')\n",
    "print(classification_report(pred1,y_val))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Resampling\n",
    "\n",
    "As we have got always poor results in prediciting the minority class, we can try to resample the data.\n",
    "We can try to increase it up to 30%.\n",
    "As we can see, we got the best results for precision, recall and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamp = SMOTE(sampling_strategy = 0.35)\n",
    "undersamp = RandomUnderSampler(sampling_strategy = 0.45)\n",
    "pipeline = Pipeline(steps = [('over', oversamp),('under', undersamp)])\n",
    "\n",
    "X, y = pipeline.fit_resample(df_prep.drop(['def_next_month'],axis=1),df_prep.def_next_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.310350146537097\n"
     ]
    }
   ],
   "source": [
    "print(y.sum()/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampled dataset\n",
    "resampled_df = pd.concat([X, y],axis=1)\n",
    "\n",
    "# Splitting Train, test dataset\n",
    "\n",
    "X_train, X_NOT_train, y_train, y_NOT_train = train_test_split(resampled_df.drop(['def_next_month'],axis=1),\n",
    "                                                                        resampled_df.def_next_month,test_size=0.3,\n",
    "                                                                        random_state=101)\n",
    "\n",
    "# split 30% groups into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_NOT_train,y_NOT_train,test_size=0.5,\n",
    "                                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Resampled\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79      2651\n",
      "           1       0.54      0.52      0.53      1239\n",
      "\n",
      "    accuracy                           0.71      3890\n",
      "   macro avg       0.66      0.66      0.66      3890\n",
      "weighted avg       0.70      0.71      0.70      3890\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree1 = DecisionTreeClassifier()\n",
    "tree1.fit(X_train, y_train)\n",
    "pred1 = tree1.predict(X_val)\n",
    "print('Decision Tree - Resampled')\n",
    "print(classification_report(pred1,y_val))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "We tried to classify the credit card default using Random Forest, KNN and decision tree models.\n",
    "We had the best results when we used the resampled data. This is probably the path to follow: performing more modeling on the resampled data.\n",
    "\n",
    "As we saw in the data exploratory part, there were many predictors correlated among each other. A further experiment might be trying to drop them and keep only th ones with low correlation.\n",
    "Lastly, we saw there were outliers. We could try to rerun the models without the outliers, or replacing them with the 5th and 95th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
